<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Pytorch中的张量 | Touchsky's Blog</title><meta name="author" content="touchsky"><meta name="copyright" content="touchsky"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="定义 张量(tensor)是一组具有多维度结构的数。维度的个数称为 阶(rank),通过tensor.dim()查看。张量的具体形状通过tensor.shape()查看。  索引方式  切片索引 像 python 的 list 和 numpy 的 array 一样，张量可以通过切片索引。PyTorch 张量可以使用 start:stop 或 start:stop:step 这样的语法进行切片。s"><meta property="og:type" content="article"><meta property="og:title" content="Pytorch中的张量"><meta property="og:url" content="http://example.com/2025/04/26/tensor-operation/index.html"><meta property="og:site_name" content="Touchsky&#39;s Blog"><meta property="og:description" content="定义 张量(tensor)是一组具有多维度结构的数。维度的个数称为 阶(rank),通过tensor.dim()查看。张量的具体形状通过tensor.shape()查看。  索引方式  切片索引 像 python 的 list 和 numpy 的 array 一样，张量可以通过切片索引。PyTorch 张量可以使用 start:stop 或 start:stop:step 这样的语法进行切片。s"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/img/avatar.webp"><meta property="article:published_time" content="2025-04-26T09:08:24.000Z"><meta property="article:modified_time" content="2025-04-26T11:59:15.535Z"><meta property="article:author" content="touchsky"><meta property="article:tag" content="深度学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://example.com/img/avatar.webp"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Pytorch中的张量",
  "url": "http://example.com/2025/04/26/tensor-operation/",
  "image": "http://example.com/img/avatar.webp",
  "datePublished": "2025-04-26T09:08:24.000Z",
  "dateModified": "2025-04-26T11:59:15.535Z",
  "author": [
    {
      "@type": "Person",
      "name": "touchsky",
      "url": "http://example.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/myfavicon.ico"><link rel="canonical" href="http://example.com/2025/04/26/tensor-operation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="google-site-verification" content="nvbYwWoql9FnuJMeqjPZGGkoiCjugINhdqpZgukPv3w"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach(([e,t])=>n.setAttribute(e,t)),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),getCSS:(e,t)=>new Promise((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),addGlobalFn:(e,t,o=!1,a=window)=>{if(e.startsWith("pjax"))return;const n=a.globalFn||{};n[e]=n[e]||{},n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!1},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyloadPlugin:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Pytorch中的张量",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Touchsky's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Pytorch中的张量</span></a></span><div id="menus"></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Pytorch中的张量</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-04-26T09:08:24.000Z" title="发表于 2025-04-26 17:08:24">2025-04-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-04-26T11:59:15.535Z" title="更新于 2025-04-26 19:59:15">2025-04-26</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><h1 id="定义"><a class="markdownIt-Anchor" href="#定义"></a> 定义</h1><p>张量(tensor)是一组具有多维度结构的数。维度的个数称为 <strong>阶</strong>(rank),通过<code>tensor.dim()查看</code>。张量的具体形状通过<code>tensor.shape()</code>查看。</p><h1 id="索引方式"><a class="markdownIt-Anchor" href="#索引方式"></a> 索引方式</h1><h2 id="切片索引"><a class="markdownIt-Anchor" href="#切片索引"></a> 切片索引</h2><p>像 python 的 list 和 numpy 的 array 一样，张量可以通过切片索引。PyTorch 张量可以使用 <code>start:stop</code> 或 <code>start:stop:step</code> 这样的语法进行切片。stop 总是不被包括在内的第一个元素。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">0</span>, <span class="number">11</span>, <span class="number">22</span>, <span class="number">33</span>, <span class="number">44</span>, <span class="number">55</span>, <span class="number">66</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="number">0</span>, a)        <span class="comment"># (0) Original tensor</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">1</span>, a[<span class="number">2</span>:<span class="number">5</span>])   <span class="comment"># (1) Elements between index 2 and 5</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">2</span>, a[<span class="number">2</span>:])    <span class="comment"># (2) Elements after index 2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">3</span>, a[:<span class="number">5</span>])    <span class="comment"># (3) Elements before index 5</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">4</span>, a[:])     <span class="comment"># (4) All elements</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">5</span>, a[<span class="number">1</span>:<span class="number">5</span>:<span class="number">2</span>]) <span class="comment"># (5) Every second element between indices 1 and 5</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">6</span>, a[:-<span class="number">1</span>])   <span class="comment"># (6) All but the last element</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">7</span>, a[-<span class="number">4</span>::<span class="number">2</span>]) <span class="comment"># (7) Every second element, starting from the fourth-last</span></span><br></pre></td></tr></table></figure><p>访问张量中的单行或单列有两种常见方法：</p><ul><li>使用一个整数索引会使张量的阶（rank）减少 1；</li><li>使用一个长度为 1 的切片（slice）则保持张量的阶不变。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], [<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>], [<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line">row_r1 = a[<span class="number">1</span>, :]    <span class="comment"># Rank 1 view of the second row of a      tensor([5, 6, 7, 8]) torch.Size([4])</span></span><br><span class="line">row_r2 = a[<span class="number">1</span>:<span class="number">2</span>, :]  <span class="comment"># Rank 2 view of the second row of a      tensor([[5, 6, 7, 8]]) torch.Size([1, 4])</span></span><br></pre></td></tr></table></figure><h2 id="整数张量索引"><a class="markdownIt-Anchor" href="#整数张量索引"></a> 整数张量索引</h2><p>整数索引可以互换张量行和列的顺序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a new tensor of shape (5, 4) by reordering rows from a:</span></span><br><span class="line"><span class="comment"># - First two rows same as the first row of a</span></span><br><span class="line"><span class="comment"># - Third row is the same as the last row of a</span></span><br><span class="line"><span class="comment"># - Fourth and fifth rows are the same as the second row from a</span></span><br><span class="line">idx = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>]  <span class="comment"># index arrays can be Python lists of integers</span></span><br><span class="line"><span class="built_in">print</span>(a[idx])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a new tensor of shape (3, 4) by reversing the columns from a</span></span><br><span class="line">idx = torch.tensor([<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>])  <span class="comment"># Index arrays can be int64 torch tensors</span></span><br><span class="line"><span class="built_in">print</span>(a[:, idx])</span><br></pre></td></tr></table></figure><p>更一般地说，给定两个索引数组 idx0 和 idx1，它们各自有 N 个元素，<br>那么表达式 a[idx0, idx1] 等价于：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([</span><br><span class="line">  a[idx0[0], idx1[0]],</span><br><span class="line">  a[idx0[1], idx1[1]],</span><br><span class="line">  ...,</span><br><span class="line">  a[idx0[N - 1], idx1[N - 1]]</span><br><span class="line">])</span><br></pre></td></tr></table></figure><h2 id="布尔张量索引"><a class="markdownIt-Anchor" href="#布尔张量索引"></a> 布尔张量索引</h2><p>布尔张量索引允许你根据一个布尔掩码（boolean mask）选取张量中任意的元素。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="comment"># Find the elements of a that are bigger than 3. The mask has the same shape as</span></span><br><span class="line"><span class="comment"># a, where each element of mask tells whether the corresponding element of a</span></span><br><span class="line"><span class="comment"># is greater than three.</span></span><br><span class="line">mask = (a &gt; <span class="number">3</span>)</span><br><span class="line"><span class="comment"># We can use the mask to construct a rank-1 tensor containing the elements of a</span></span><br><span class="line"><span class="comment"># that are selected by the mask</span></span><br><span class="line"><span class="built_in">print</span>(a[mask])</span><br><span class="line"><span class="comment"># We can also use boolean masks to modify tensors; for example this sets all</span></span><br><span class="line"><span class="comment"># elements &lt;= 3 to zero:</span></span><br><span class="line">a[a &lt;= <span class="number">3</span>] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure><p><img src="/2025/04/26/tensor-operation/boolindex.png" alt="布尔张量索引"></p><h1 id="张量操作"><a class="markdownIt-Anchor" href="#张量操作"></a> 张量操作</h1><h2 id="按元素操作"><a class="markdownIt-Anchor" href="#按元素操作"></a> 按元素操作</h2><p>基本的数学函数在张量上是按元素（elementwise）操作的，可以通过运算符重载,torch 模块中的函数，以及张量对象的方法来使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]], dtype=torch.float32)</span><br><span class="line">y = torch.tensor([[<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Elementwise sum; all give the same result</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Elementwise sum:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(x + y)</span><br><span class="line"><span class="built_in">print</span>(torch.add(x, y))</span><br><span class="line"><span class="built_in">print</span>(x.add(y))</span><br><span class="line"></span><br><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]], dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(torch.sqrt(x))</span><br><span class="line"><span class="built_in">print</span>(x.sqrt())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.sin(x))</span><br><span class="line"><span class="built_in">print</span>(x.sin())</span><br><span class="line"><span class="built_in">print</span>(torch.cos(x))</span><br><span class="line"><span class="built_in">print</span>(x.cos())</span><br></pre></td></tr></table></figure><h1 id="广播"><a class="markdownIt-Anchor" href="#广播"></a> 广播</h1><p>广播（Broadcasting）是一种强大的机制，它允许 PyTorch 在执行算术运算时处理形状不同的数组。</p><p>将两个张量进行广播（broadcasting）时，遵循以下规则：</p><ol><li><p>如果两个张量的阶（rank）不同，先在较低阶张量的形状（shape）<strong>前面</strong>补 1，直到两个形状长度相同。</p></li><li><p>如果在某个维度上，两者的大小相同，或者其中一个张量在该维度上的大小为 1，那么这两个张量在这个维度上被认为是<strong>兼容</strong>的。</p></li><li><p>只有当两个张量在<strong>所有</strong>维度上都兼容时，它们才能进行广播。</p></li><li><p>广播之后，每个张量的行为就好像它的形状是两个输入张量形状在各个维度上取元素最大值的结果。</p></li><li><p>在任何一个维度上，如果一个张量的大小是 1，另一个张量的大小大于 1，那么这个大小为 1 的张量在该维度上表现得像是被复制了一样。</p></li></ol></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">touchsky</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/04/26/tensor-operation/">http://example.com/2025/04/26/tensor-operation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">Touchsky's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post-share"><div class="social-share" data-image="/img/avatar.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/04/10/deeplearning-note-part2/" title="深度学习笔记(第二部分)"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">深度学习笔记(第二部分)</div></div><div class="info-2"><div class="info-item-1">循环神经网络 (Recurrent Networks) 应用场景 之前 CNN 和 MLP 的输入和输出都只有一个,但实际问题中的输入和输出可能有多个。比如给图像加描述(one to many), 视频分类(many to one), 机器翻译(many to many),这些应用中可以使用循环神经网络。 当你处理的问题的输入或输出涉及到 序列 时候，可以使用循环神经网络。 循环神经网络可以对非序列型的数据比如图片进行序列化的处理。 基础概念 我们可以通过在每一个时间步应用一个递推公式来处理一系列向量xxx。 其中使用相同的权重矩阵WWW和状态更新函数fWf_WfW​就可以处理任意长的序列。 在序列被处理过程中，RNN 有一个一直更新的内部状态。 ht=fW(ht−1,xt)h_{t}=f_{W} ( h_{t-1}, x_{t} ) ht​=fW​(ht−1​,xt​) 初始状态h0h_0h0​被设置为全 0 或者通过学习得到。 RNN 实现 Vanilla RNN 一种 RNN 是&quot;Vanilla...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/03/29/deeplearning-note-part1/" title="深度学习笔记(第一部分)"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-29</div><div class="info-item-2">深度学习笔记(第一部分)</div></div><div class="info-2"><div class="info-item-1">线性分类器（Linear Classifiers） 线性分类器的缺点 解决方法之一：特征变换 优化（Optimization） w∗=arg⁡min⁡wL(w)w^{*}=\operatorname{a r g} \operatorname* {m i n}_{w} L ( w ) w∗=argwmin​L(w) SGD 对 gradient descent 进行 Stochastic 处理，每次迭代时候抽取一批样本而不是用全部样本用于参数更新来降低算力要求。 xt+1=xt−α∇f(xt)x_{t+1}=x_{t}-\alpha\nabla f ( x_{t} ) xt+1​=xt​−α∇f(xt​) 123for t in range(num_steps): dw = compute_gradient(w) w -= learning_rate * dw 问题 SGD with Momentum SGD with Momentum 是为了克服 SGD 在收敛的过程中可能会停在 局部最小值 或者 鞍点 的问题，在这些点处梯度为...</div></div></div></a><a class="pagination-related" href="/2025/04/10/deeplearning-note-part2/" title="深度学习笔记(第二部分)"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-10</div><div class="info-item-2">深度学习笔记(第二部分)</div></div><div class="info-2"><div class="info-item-1">循环神经网络 (Recurrent Networks) 应用场景 之前 CNN 和 MLP 的输入和输出都只有一个,但实际问题中的输入和输出可能有多个。比如给图像加描述(one to many), 视频分类(many to one), 机器翻译(many to many),这些应用中可以使用循环神经网络。 当你处理的问题的输入或输出涉及到 序列 时候，可以使用循环神经网络。 循环神经网络可以对非序列型的数据比如图片进行序列化的处理。 基础概念 我们可以通过在每一个时间步应用一个递推公式来处理一系列向量xxx。 其中使用相同的权重矩阵WWW和状态更新函数fWf_WfW​就可以处理任意长的序列。 在序列被处理过程中，RNN 有一个一直更新的内部状态。 ht=fW(ht−1,xt)h_{t}=f_{W} ( h_{t-1}, x_{t} ) ht​=fW​(ht−1​,xt​) 初始状态h0h_0h0​被设置为全 0 或者通过学习得到。 RNN 实现 Vanilla RNN 一种 RNN 是&quot;Vanilla...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.webp" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">touchsky</div><div class="author-info-description">学习笔记、技术问题记录</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">3</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/touchsky-real" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E6%96%B9%E5%BC%8F"><span class="toc-number">2.</span> <span class="toc-text">索引方式</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%87%E7%89%87%E7%B4%A2%E5%BC%95"><span class="toc-number">2.1.</span> <span class="toc-text">切片索引</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B4%E6%95%B0%E5%BC%A0%E9%87%8F%E7%B4%A2%E5%BC%95"><span class="toc-number">2.2.</span> <span class="toc-text">整数张量索引</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%83%E5%B0%94%E5%BC%A0%E9%87%8F%E7%B4%A2%E5%BC%95"><span class="toc-number">2.3.</span> <span class="toc-text">布尔张量索引</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C"><span class="toc-number">3.</span> <span class="toc-text">张量操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8C%89%E5%85%83%E7%B4%A0%E6%93%8D%E4%BD%9C"><span class="toc-number">3.1.</span> <span class="toc-text">按元素操作</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD"><span class="toc-number">4.</span> <span class="toc-text">广播</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/26/tensor-operation/" title="Pytorch中的张量">Pytorch中的张量</a><time datetime="2025-04-26T09:08:24.000Z" title="发表于 2025-04-26 17:08:24">2025-04-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/10/deeplearning-note-part2/" title="深度学习笔记(第二部分)">深度学习笔记(第二部分)</a><time datetime="2025-04-10T12:40:16.000Z" title="发表于 2025-04-10 20:40:16">2025-04-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/29/deeplearning-note-part1/" title="深度学习笔记(第一部分)">深度学习笔记(第一部分)</a><time datetime="2025-03-29T12:59:13.000Z" title="发表于 2025-03-29 20:59:13">2025-03-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By touchsky</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async()=>{window.katex_js_css||(window.katex_js_css=!0,await btf.getCSS("https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"),await btf.getScript("https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js")),document.querySelectorAll("#article-container .katex").forEach(t=>t.classList.add("katex-show"))})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>