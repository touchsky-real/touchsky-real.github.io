<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Pytorch中的张量 | Touchsky's Blog</title><meta name="author" content="touchsky"><meta name="copyright" content="touchsky"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="定义 张量(tensor)是一组具有多维度结构的数。维度的个数称为 阶(rank),通过tensor.dim()查看。张量的具体形状通过tensor.shape()查看。  索引方式  切片索引 像 python 的 list 和 numpy 的 array 一样，张量可以使用 start:stop 或 start:stop:step 这样的语法进行切片。stop 是不被包括在内的第一个元素。"><meta property="og:type" content="article"><meta property="og:title" content="Pytorch中的张量"><meta property="og:url" content="http://example.com/2025/04/26/tensor-operation/index.html"><meta property="og:site_name" content="Touchsky&#39;s Blog"><meta property="og:description" content="定义 张量(tensor)是一组具有多维度结构的数。维度的个数称为 阶(rank),通过tensor.dim()查看。张量的具体形状通过tensor.shape()查看。  索引方式  切片索引 像 python 的 list 和 numpy 的 array 一样，张量可以使用 start:stop 或 start:stop:step 这样的语法进行切片。stop 是不被包括在内的第一个元素。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/img/avatar.webp"><meta property="article:published_time" content="2025-04-26T09:08:24.000Z"><meta property="article:modified_time" content="2025-04-27T15:50:59.509Z"><meta property="article:author" content="touchsky"><meta property="article:tag" content="深度学习"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://example.com/img/avatar.webp"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Pytorch中的张量",
  "url": "http://example.com/2025/04/26/tensor-operation/",
  "image": "http://example.com/img/avatar.webp",
  "datePublished": "2025-04-26T09:08:24.000Z",
  "dateModified": "2025-04-27T15:50:59.509Z",
  "author": [
    {
      "@type": "Person",
      "name": "touchsky",
      "url": "http://example.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/myfavicon.ico"><link rel="canonical" href="http://example.com/2025/04/26/tensor-operation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="google-site-verification" content="nvbYwWoql9FnuJMeqjPZGGkoiCjugINhdqpZgukPv3w"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach(([e,t])=>n.setAttribute(e,t)),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),getCSS:(e,t)=>new Promise((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),addGlobalFn:(e,t,o=!1,a=window)=>{if(e.startsWith("pjax"))return;const n=a.globalFn||{};n[e]=n[e]||{},n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!1},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyloadPlugin:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Pytorch中的张量",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Touchsky's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Pytorch中的张量</span></a></span><div id="menus"></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Pytorch中的张量</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-04-26T09:08:24.000Z" title="发表于 2025-04-26 17:08:24">2025-04-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-04-27T15:50:59.509Z" title="更新于 2025-04-27 23:50:59">2025-04-27</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><h1 id="定义"><a class="markdownIt-Anchor" href="#定义"></a> 定义</h1><p>张量(tensor)是一组具有多维度结构的数。维度的个数称为 <strong>阶</strong>(rank),通过<code>tensor.dim()</code>查看。张量的具体形状通过<code>tensor.shape()</code>查看。</p><h1 id="索引方式"><a class="markdownIt-Anchor" href="#索引方式"></a> 索引方式</h1><h2 id="切片索引"><a class="markdownIt-Anchor" href="#切片索引"></a> 切片索引</h2><p>像 python 的 list 和 numpy 的 array 一样，张量可以使用 <code>start:stop</code> 或 <code>start:stop:step</code> 这样的语法进行切片。stop 是不被包括在内的第一个元素。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">0</span>, <span class="number">11</span>, <span class="number">22</span>, <span class="number">33</span>, <span class="number">44</span>, <span class="number">55</span>, <span class="number">66</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="number">0</span>, a)        <span class="comment"># (0) Original tensor</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">1</span>, a[<span class="number">2</span>:<span class="number">5</span>])   <span class="comment"># (1) Elements between index 2 and 5</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">2</span>, a[<span class="number">2</span>:])    <span class="comment"># (2) Elements after index 2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">3</span>, a[:<span class="number">5</span>])    <span class="comment"># (3) Elements before index 5</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">4</span>, a[:])     <span class="comment"># (4) All elements</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">5</span>, a[<span class="number">1</span>:<span class="number">5</span>:<span class="number">2</span>]) <span class="comment"># (5) Every second element between indices 1 and 5</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">6</span>, a[:-<span class="number">1</span>])   <span class="comment"># (6) All but the last element</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">7</span>, a[-<span class="number">4</span>::<span class="number">2</span>]) <span class="comment"># (7) Every second element, starting from the fourth-last</span></span><br></pre></td></tr></table></figure><p>访问张量中的单行或单列有两种常见方法：</p><ul><li>使用一个整数索引会使张量的阶（rank）减少 1。</li><li>使用一个长度为 1 的切片（slice）则保持张量的阶不变。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], [<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>], [<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line">row_r1 = a[<span class="number">1</span>, :]    <span class="comment"># Rank 1 view of the second row of a      tensor([5, 6, 7, 8]) torch.Size([4])</span></span><br><span class="line">row_r2 = a[<span class="number">1</span>:<span class="number">2</span>, :]  <span class="comment"># Rank 2 view of the second row of a      tensor([[5, 6, 7, 8]]) torch.Size([1, 4])</span></span><br></pre></td></tr></table></figure><h2 id="整数张量索引"><a class="markdownIt-Anchor" href="#整数张量索引"></a> 整数张量索引</h2><p>整数索引可以互换张量行和列的顺序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a new tensor of shape (5, 4) by reordering rows from a:</span></span><br><span class="line"><span class="comment"># - First two rows same as the first row of a</span></span><br><span class="line"><span class="comment"># - Third row is the same as the last row of a</span></span><br><span class="line"><span class="comment"># - Fourth and fifth rows are the same as the second row from a</span></span><br><span class="line">idx = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>]  <span class="comment"># index arrays can be Python lists of integers</span></span><br><span class="line"><span class="built_in">print</span>(a[idx])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a new tensor of shape (3, 4) by reversing the columns from a</span></span><br><span class="line">idx = torch.tensor([<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>])  <span class="comment"># Index arrays can be int64 torch tensors</span></span><br><span class="line"><span class="built_in">print</span>(a[:, idx])</span><br></pre></td></tr></table></figure><p>更一般地说，给定两个索引数组 <code>idx0</code> 和 <code>idx1</code>，它们各自有 N 个元素，那么表达式 <code>a[idx0, idx1]</code> 等价于：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([</span><br><span class="line">  a[idx0[0], idx1[0]],</span><br><span class="line">  a[idx0[1], idx1[1]],</span><br><span class="line">  ...,</span><br><span class="line">  a[idx0[N - 1], idx1[N - 1]]</span><br><span class="line">])</span><br></pre></td></tr></table></figure><h2 id="布尔张量索引"><a class="markdownIt-Anchor" href="#布尔张量索引"></a> 布尔张量索引</h2><p>布尔张量索引允许你根据一个布尔掩码（boolean mask）选取张量中任意的元素。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="comment"># Find the elements of a that are bigger than 3. The mask has the same shape as</span></span><br><span class="line"><span class="comment"># a, where each element of mask tells whether the corresponding element of a</span></span><br><span class="line"><span class="comment"># is greater than three.</span></span><br><span class="line">mask = (a &gt; <span class="number">3</span>)</span><br><span class="line"><span class="comment"># We can use the mask to construct a rank-1 tensor containing the elements of a</span></span><br><span class="line"><span class="comment"># that are selected by the mask</span></span><br><span class="line"><span class="built_in">print</span>(a[mask])</span><br><span class="line"><span class="comment"># We can also use boolean masks to modify tensors; for example this sets all</span></span><br><span class="line"><span class="comment"># elements &lt;= 3 to zero:</span></span><br><span class="line">a[a &lt;= <span class="number">3</span>] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure><p><img src="/2025/04/26/tensor-operation/boolindex.png" alt="布尔张量索引"></p><h1 id="张量操作"><a class="markdownIt-Anchor" href="#张量操作"></a> 张量操作</h1><h2 id="按元素操作"><a class="markdownIt-Anchor" href="#按元素操作"></a> 按元素操作</h2><p>基本的数学函数在张量上是按元素（elementwise）操作的，可以通过运算符重载,torch 模块中的函数，以及张量对象的方法来使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]], dtype=torch.float32)</span><br><span class="line">y = torch.tensor([[<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Elementwise sum; all give the same result</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Elementwise sum:&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(x + y)</span><br><span class="line"><span class="built_in">print</span>(torch.add(x, y))</span><br><span class="line"><span class="built_in">print</span>(x.add(y))</span><br><span class="line"></span><br><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]], dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(torch.sqrt(x))</span><br><span class="line"><span class="built_in">print</span>(x.sqrt())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.sin(x))</span><br><span class="line"><span class="built_in">print</span>(x.sin())</span><br><span class="line"><span class="built_in">print</span>(torch.cos(x))</span><br><span class="line"><span class="built_in">print</span>(x.cos())</span><br></pre></td></tr></table></figure><h2 id="reduction-操作"><a class="markdownIt-Anchor" href="#reduction-操作"></a> reduction 操作</h2><p>我们有时希望对张量的部分或全部元素执行聚合操作，比如求和,这类操作被称为 reduction 操作。reduction 操作包括求和(sum)、求平均值(mean)、取最小值(min)、取最大值(max)等。</p><p>reduction 操作降低张量的阶。传入参数 dim=d 后，输入张量中索引为 d 的维度会从输出张量的形状中移除。如果传入参数 keepdim=True，则指定的维度不会被移除而是变为 1。在进行 reduction 操作时，考虑张量的形状比较便于理解。</p><p>有的 reduction 操作返回不止一个值，比如 min 会返回指定维度上的最小值，以及最小值所在的位置索引：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个张量</span></span><br><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">lresult = x.<span class="built_in">min</span>(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># result是一个命名元组，有两个属性：</span></span><br><span class="line"><span class="built_in">print</span>(result.values)    <span class="comment"># 获取最小值</span></span><br><span class="line"><span class="built_in">print</span>(result.indices)   <span class="comment"># 获取最小值的索引位置</span></span><br><span class="line"><span class="comment"># 或者解包</span></span><br><span class="line">col_min_vals, col_min_idxs = x.<span class="built_in">min</span>(dim=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>以上的<code>torch.min</code>输入的参数一般是<strong>一个</strong>张量。有另一个函数<code>torch.minimum</code>用于按元素比较<strong>两个</strong> tensor，返回对应元素中较小的那个。类似的还有<code>torch.maximum</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>])</span><br><span class="line">b = torch.tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">torch.minimum(a, b)</span><br><span class="line"><span class="comment"># 输出: tensor([1, 2, 3])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="reshaping-操作"><a class="markdownIt-Anchor" href="#reshaping-操作"></a> Reshaping 操作</h2><h3 id="view"><a class="markdownIt-Anchor" href="#view"></a> view</h3><p>PyTorch 提供了多种操作张量形状的方法。比如<code>view()</code>返回一个与输入具有相同元素数量，但形状不同的新张量。</p><p><code>view()</code> 按照原张量的<strong>行优先</strong>顺序来重新排列元素，而张量在内存中存储也是<strong>行优先</strong>的。</p><p>正如其名称所暗示，通过<code>view()</code>返回的张量与输入共享相同的数据，因此对其中一个的更改将影响另一个。</p><p>我们可以使用<code>view()</code>将矩阵展平为向量，将一维向量转换为二维的行矩阵或列矩阵。</p><h3 id="unsqueeze"><a class="markdownIt-Anchor" href="#unsqueeze"></a> unsqueeze</h3><p><code>unsqueeze</code>在指定位置插入一个维度,它有一种简便写法。<br><code>a[:, None]</code>的意思是在原张量的后面新增一个维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">c = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式1：用 None</span></span><br><span class="line">a = c[:, <span class="literal">None</span>]</span><br><span class="line"><span class="built_in">print</span>(a.shape)  <span class="comment"># torch.Size([3, 1])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式2：用 unsqueeze</span></span><br><span class="line">b = c.unsqueeze(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(b.shape)  <span class="comment"># torch.Size([3, 1])</span></span><br></pre></td></tr></table></figure><h3 id="cat-与-stack"><a class="markdownIt-Anchor" href="#cat-与-stack"></a> cat 与 stack</h3><p>torch.cat 沿现有的维度拼接张量，不会增加新的维度。<br>如果你有两个形状为 (2, 3) 的张量，使用 torch.cat 沿着第 0 维（行）拼接后，结果张量的形状将是 (4, 3)</p><p>torch.stack 沿一个新的维度堆叠张量，会增加一个新的维度。<br>如果你有两个形状为 (2, 3) 的张量，使用 torch.stack 沿新的第 0 维堆叠结果张量的形状将是 (2, 2, 3)</p><h2 id="张量与标量的计算"><a class="markdownIt-Anchor" href="#张量与标量的计算"></a> 张量与标量的计算</h2><p>张量和标量（Scalar）可以直接进行运算，而且操作是元素级（element-wise）的，PyTorch 会自动广播标量到张量的每个元素。比如+、-、*、/、<strong>==</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tensor = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>])</span><br><span class="line">scalar = <span class="number">2.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加法</span></span><br><span class="line">result_add = tensor + scalar  <span class="comment"># 每个元素加2</span></span><br><span class="line"><span class="built_in">print</span>(result_add)  <span class="comment"># tensor([3., 4., 5.])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 乘法</span></span><br><span class="line">result_mul = tensor * scalar  <span class="comment"># 每个元素乘2</span></span><br><span class="line"><span class="built_in">print</span>(result_mul)  <span class="comment"># tensor([2., 4., 6.])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 减法</span></span><br><span class="line">result_sub = tensor - scalar</span><br><span class="line"><span class="built_in">print</span>(result_sub)  <span class="comment"># tensor([-1., 0., 1.])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 除法</span></span><br><span class="line">result_div = tensor / scalar</span><br><span class="line"><span class="built_in">print</span>(result_div)  <span class="comment"># tensor([0.5, 1.0, 1.5])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较</span></span><br><span class="line">result = tensor == scalar</span><br><span class="line"><span class="built_in">print</span>(result)  <span class="comment"># tensor([False,  True, False])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="广播"><a class="markdownIt-Anchor" href="#广播"></a> 广播</h1><p>广播（Broadcasting）是一种强大的机制，它允许 PyTorch 在执行算术运算时处理形状不同的数组。</p><p>将两个张量进行广播（broadcasting）时，遵循以下规则：</p><ol><li><p>如果两个张量的阶（rank）不同，先在较低阶张量的形状（shape）<strong>前面</strong>补 1，直到两个形状长度相同。</p></li><li><p>如果在某个维度上，两者的大小相同，或者其中一个张量在该维度上的大小为 1，那么这两个张量在这个维度上被认为是<strong>兼容</strong>的。</p></li><li><p>只有当两个张量在<strong>所有</strong>维度上都兼容时，它们才能进行广播。</p></li><li><p>广播之后，每个张量的形状是两个输入张量形状在逐个维度上的最大值。</p></li><li><p>在任何一个维度上，如果一个张量的大小是 1，另一个张量的大小大于 1，那么这个大小为 1 的张量在该维度上表现得像是被复制了一样。</p></li></ol><h1 id="其他"><a class="markdownIt-Anchor" href="#其他"></a> 其他</h1><h2 id="隐式类型转换"><a class="markdownIt-Anchor" href="#隐式类型转换"></a> 隐式类型转换</h2><p>布尔张量与浮点数、整数运算时自动转换为 0 和 1。</p><p>整数和浮点数运算时,结果为浮点数。不同精度浮点数运算时,结果为更高精度。</p><h2 id="设备"><a class="markdownIt-Anchor" href="#设备"></a> 设备</h2><p>训练过程中模型和数据必须在同一个设备上，否则会报错。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model = model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloader:</span><br><span class="line">    inputs = inputs.to(device)</span><br><span class="line">    labels = labels.to(device)</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line"></span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">touchsky</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/04/26/tensor-operation/">http://example.com/2025/04/26/tensor-operation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">Touchsky's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post-share"><div class="social-share" data-image="/img/avatar.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/04/10/deeplearning-note-part2/" title="深度学习笔记(第二部分)"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">深度学习笔记(第二部分)</div></div><div class="info-2"><div class="info-item-1">循环神经网络 (Recurrent Networks) 应用场景 之前 CNN 和 MLP 的输入和输出都只有一个,但实际问题中的输入和输出可能有多个。比如给图像加描述(one to many), 视频分类(many to one), 机器翻译(many to many),这些应用中可以使用循环神经网络。 当你处理的问题的输入或输出涉及到 序列 时候，可以使用循环神经网络。 循环神经网络可以对非序列型的数据比如图片进行序列化的处理。 基础概念 我们可以通过在每一个时间步应用一个递推公式来处理一系列向量xxx。 其中使用相同的权重矩阵WWW和状态更新函数fWf_WfW​就可以处理任意长的序列。 在序列被处理过程中，RNN 有一个一直更新的内部状态。 ht=fW(ht−1,xt)h_{t}=f_{W} ( h_{t-1}, x_{t} ) ht​=fW​(ht−1​,xt​) 初始状态h0h_0h0​被设置为全 0 或者通过学习得到。 RNN 实现 Vanilla RNN 一种 RNN 是&quot;Vanilla...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/03/29/deeplearning-note-part1/" title="深度学习笔记(第一部分)"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-29</div><div class="info-item-2">深度学习笔记(第一部分)</div></div><div class="info-2"><div class="info-item-1">线性分类器（Linear Classifiers） 线性分类器的缺点 解决方法之一：特征变换 优化（Optimization） w∗=arg⁡min⁡wL(w)w^{*}=\operatorname{a r g} \operatorname* {m i n}_{w} L ( w ) w∗=argwmin​L(w) SGD 对 gradient descent 进行 Stochastic 处理，每次迭代时候抽取一批样本而不是用全部样本用于参数更新来降低算力要求。 xt+1=xt−α∇f(xt)x_{t+1}=x_{t}-\alpha\nabla f ( x_{t} ) xt+1​=xt​−α∇f(xt​) 123for t in range(num_steps): dw = compute_gradient(w) w -= learning_rate * dw 问题 SGD with Momentum SGD with Momentum 是为了克服 SGD 在收敛的过程中可能会停在 局部最小值 或者 鞍点 的问题，在这些点处梯度为...</div></div></div></a><a class="pagination-related" href="/2025/04/10/deeplearning-note-part2/" title="深度学习笔记(第二部分)"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-10</div><div class="info-item-2">深度学习笔记(第二部分)</div></div><div class="info-2"><div class="info-item-1">循环神经网络 (Recurrent Networks) 应用场景 之前 CNN 和 MLP 的输入和输出都只有一个,但实际问题中的输入和输出可能有多个。比如给图像加描述(one to many), 视频分类(many to one), 机器翻译(many to many),这些应用中可以使用循环神经网络。 当你处理的问题的输入或输出涉及到 序列 时候，可以使用循环神经网络。 循环神经网络可以对非序列型的数据比如图片进行序列化的处理。 基础概念 我们可以通过在每一个时间步应用一个递推公式来处理一系列向量xxx。 其中使用相同的权重矩阵WWW和状态更新函数fWf_WfW​就可以处理任意长的序列。 在序列被处理过程中，RNN 有一个一直更新的内部状态。 ht=fW(ht−1,xt)h_{t}=f_{W} ( h_{t-1}, x_{t} ) ht​=fW​(ht−1​,xt​) 初始状态h0h_0h0​被设置为全 0 或者通过学习得到。 RNN 实现 Vanilla RNN 一种 RNN 是&quot;Vanilla...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.webp" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">touchsky</div><div class="author-info-description">学习笔记、技术问题记录</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">3</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/touchsky-real" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E6%96%B9%E5%BC%8F"><span class="toc-number">2.</span> <span class="toc-text">索引方式</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%87%E7%89%87%E7%B4%A2%E5%BC%95"><span class="toc-number">2.1.</span> <span class="toc-text">切片索引</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B4%E6%95%B0%E5%BC%A0%E9%87%8F%E7%B4%A2%E5%BC%95"><span class="toc-number">2.2.</span> <span class="toc-text">整数张量索引</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%83%E5%B0%94%E5%BC%A0%E9%87%8F%E7%B4%A2%E5%BC%95"><span class="toc-number">2.3.</span> <span class="toc-text">布尔张量索引</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C"><span class="toc-number">3.</span> <span class="toc-text">张量操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8C%89%E5%85%83%E7%B4%A0%E6%93%8D%E4%BD%9C"><span class="toc-number">3.1.</span> <span class="toc-text">按元素操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reduction-%E6%93%8D%E4%BD%9C"><span class="toc-number">3.2.</span> <span class="toc-text">reduction 操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reshaping-%E6%93%8D%E4%BD%9C"><span class="toc-number">3.3.</span> <span class="toc-text">Reshaping 操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#view"><span class="toc-number">3.3.1.</span> <span class="toc-text">view</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#unsqueeze"><span class="toc-number">3.3.2.</span> <span class="toc-text">unsqueeze</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cat-%E4%B8%8E-stack"><span class="toc-number">3.3.3.</span> <span class="toc-text">cat 与 stack</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E4%B8%8E%E6%A0%87%E9%87%8F%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="toc-number">3.4.</span> <span class="toc-text">张量与标量的计算</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD"><span class="toc-number">4.</span> <span class="toc-text">广播</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">5.</span> <span class="toc-text">其他</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%90%E5%BC%8F%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2"><span class="toc-number">5.1.</span> <span class="toc-text">隐式类型转换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BE%E5%A4%87"><span class="toc-number">5.2.</span> <span class="toc-text">设备</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/26/tensor-operation/" title="Pytorch中的张量">Pytorch中的张量</a><time datetime="2025-04-26T09:08:24.000Z" title="发表于 2025-04-26 17:08:24">2025-04-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/10/deeplearning-note-part2/" title="深度学习笔记(第二部分)">深度学习笔记(第二部分)</a><time datetime="2025-04-10T12:40:16.000Z" title="发表于 2025-04-10 20:40:16">2025-04-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/29/deeplearning-note-part1/" title="深度学习笔记(第一部分)">深度学习笔记(第一部分)</a><time datetime="2025-03-29T12:59:13.000Z" title="发表于 2025-03-29 20:59:13">2025-03-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By touchsky</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async()=>{window.katex_js_css||(window.katex_js_css=!0,await btf.getCSS("https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"),await btf.getScript("https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js")),document.querySelectorAll("#article-container .katex").forEach(t=>t.classList.add("katex-show"))})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>