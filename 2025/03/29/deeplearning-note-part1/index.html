<!DOCTYPE html><html lang="zh" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>深度学习笔记(第一部分) | Touchsky's blog</title><meta name="author" content="touchsky"><meta name="copyright" content="touchsky"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="线性分类器（Linear Classifiers）线性分类器的缺点 解决方法之一：特征变换 优化（Optimization） w^{*}&#x3D;\operatorname{a r g} \operatorname* {m i n}_{w} L ( w )SGD对 gradient descent 进行 Stochastic 处理，每次迭代时候抽取一批样本而不是用全部样本用于参数更新来降低算力要求。  x"><meta property="og:type" content="article"><meta property="og:title" content="深度学习笔记(第一部分)"><meta property="og:url" content="http://example.com/2025/03/29/deeplearning-note-part1/index.html"><meta property="og:site_name" content="Touchsky&#39;s blog"><meta property="og:description" content="线性分类器（Linear Classifiers）线性分类器的缺点 解决方法之一：特征变换 优化（Optimization） w^{*}&#x3D;\operatorname{a r g} \operatorname* {m i n}_{w} L ( w )SGD对 gradient descent 进行 Stochastic 处理，每次迭代时候抽取一批样本而不是用全部样本用于参数更新来降低算力要求。  x"><meta property="og:locale"><meta property="og:image" content="http://example.com/img/avatar.webp"><meta property="article:published_time" content="2025-03-29T12:59:13.000Z"><meta property="article:modified_time" content="2025-04-24T12:25:25.760Z"><meta property="article:author" content="touchsky"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://example.com/img/avatar.webp"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "深度学习笔记(第一部分)",
  "url": "http://example.com/2025/03/29/deeplearning-note-part1/",
  "image": "http://example.com/img/avatar.webp",
  "datePublished": "2025-03-29T12:59:13.000Z",
  "dateModified": "2025-04-24T12:25:25.760Z",
  "author": [
    {
      "@type": "Person",
      "name": "touchsky",
      "url": "http://example.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/03/29/deeplearning-note-part1/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>(()=>{const e={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:e,getScript:(e,t={})=>new Promise((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach(([e,t])=>n.setAttribute(e,t)),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),getCSS:(e,t)=>new Promise((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)}),addGlobalFn:(e,t,o=!1,a=window)=>{if(e.startsWith("pjax"))return;const n=a.globalFn||{};n[e]=n[e]||{},n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n}};const t=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},o=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};btf.activateDarkMode=t,btf.activateLightMode=o;const a=e.get("theme");"dark"===a?t():"light"===a&&o();const n=e.get("aside-status");void 0!==n&&document.documentElement.classList.toggle("hide-aside","hide"===n);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!1},copy:{success:"Copy Successful",error:"Copy Failed",noSupport:"Browser Not Supported"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"Just now",min:"minutes ago",hour:"hours ago",day:"days ago",month:"months ago"},copyright:void 0,lightbox:"null",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"Load More"},isPhotoFigcaption:!1,islazyloadPlugin:!1,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"深度学习笔记(第一部分)",isHighlightShrink:!1,isToc:!0,pageType:"post"}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Touchsky's blog</span></a><a class="nav-page-title" href="/"><span class="site-name">深度学习笔记(第一部分)</span></a></span><div id="menus"></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">深度学习笔记(第一部分)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-03-29T12:59:13.000Z" title="Created 2025-03-29 20:59:13">2025-03-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-04-24T12:25:25.760Z" title="Updated 2025-04-24 20:25:25">2025-04-24</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><h1 id="线性分类器（Linear-Classifiers）"><a href="#线性分类器（Linear-Classifiers）" class="headerlink" title="线性分类器（Linear Classifiers）"></a>线性分类器（Linear Classifiers）</h1><p>线性分类器的缺点<br><img src="/2025/03/29/deeplearning-note-part1/Linear%20Classifiers%20Problem.png" alt="线性分类器的缺点"></p><p>解决方法之一：<strong>特征变换</strong><br><img src="/2025/03/29/deeplearning-note-part1/特征变换.png" alt="特征变换"></p><h1 id="优化（Optimization）"><a href="#优化（Optimization）" class="headerlink" title="优化（Optimization）"></a>优化（Optimization）</h1><script type="math/tex;mode=display">w^{*}=\operatorname{a r g} \operatorname* {m i n}_{w} L ( w )</script><h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><p>对 gradient descent 进行 Stochastic 处理，每次迭代时候抽取一批样本而不是用全部样本用于参数更新来降低算力要求。</p><script type="math/tex;mode=display">x_{t+1}=x_{t}-\alpha\nabla f ( x_{t} )</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">    dw = compute_gradient(w)</span><br><span class="line">    w -= learning_rate * dw</span><br></pre></td></tr></table></figure><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p><img src="/2025/03/29/deeplearning-note-part1/SGDproblem.png" alt="problem of SGD"></p><h2 id="SGD-with-Momentum"><a href="#SGD-with-Momentum" class="headerlink" title="SGD with Momentum"></a>SGD with Momentum</h2><p>SGD with Momentum 是为了克服 SGD 在收敛的过程中可能会停在 <strong>局部最小值</strong> 或者 <strong>鞍点</strong> 的问题，在这些点处梯度为 0，参数无法继续更新。</p><p>通过给 SGD 一个速度，从而越过<strong>局部最小值</strong> 或者 <strong>鞍点</strong> 可以解决这些问题。</p><p><img src="/2025/03/29/deeplearning-note-part1/SGDwithMomentum.png" alt="SGD with Momentum"></p><script type="math/tex;mode=display">\begin{aligned}
v_{t+1} &= \rho v_{t} + \nabla f ( x_{t} ) \\
x_{t+1} &= x_{t} - \alpha v_{t+1}
\end{aligned}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">v = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">    dw = compute_gradient(w)</span><br><span class="line">    v = rho * v + dw</span><br><span class="line">    w -= learning_rate * v</span><br></pre></td></tr></table></figure><p>Build up “velocity” as a running mean of gradients. Rho gives”friction”;typically rho=0.9 or 0.99</p><p>等价于</p><script type="math/tex;mode=display">\begin{aligned}
  v_{t+1} &= \rho v_{t}-\alpha\nabla f ( x_{t} ) \\
  x_{t+1} &= x_{t}+v_{t+1}
\end{aligned}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">v = <span class="number">0</span>  <span class="comment"># 初始化动量项</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):  <span class="comment"># 迭代 num_steps 次</span></span><br><span class="line">    dw = compute_gradient(w)  <span class="comment"># 计算当前权重 w 的梯度</span></span><br><span class="line">    v = rho * v - learning_rate * dw  <span class="comment"># 计算新的动量值</span></span><br><span class="line">    w += v  <span class="comment"># 更新权重</span></span><br></pre></td></tr></table></figure><h2 id="Nesterov-Momentum"><a href="#Nesterov-Momentum" class="headerlink" title="Nesterov Momentum"></a>Nesterov Momentum</h2><p>根据速度向量到达新的点后计算梯度，对这个梯度和原来的速度进行向量和，作为原来的点更新使用的梯度。<br><img src="/2025/03/29/deeplearning-note-part1/NesterovMomentum.png" alt="Nesterov Momentum graph"></p><script type="math/tex;mode=display">\begin{array} {l} {v_{t+1}=\rho v_{t}-\alpha\nabla f ( x_{t}+\rho v_{t} )} \\ {x_{t+1}=x_{t}+v_{t+1}} \\ \end{array}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">V = <span class="number">0</span>  <span class="comment"># 初始化动量</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">    dw = compute_gradient(w)  <span class="comment"># 计算梯度</span></span><br><span class="line">    old_v = V  <span class="comment"># 记录旧的动量</span></span><br><span class="line">    V = rho * V - learning_rate * dw  <span class="comment"># 更新动量</span></span><br><span class="line">    w -= rho * old_v - (<span class="number">1</span> + rho) * V  <span class="comment"># 更新权重</span></span><br></pre></td></tr></table></figure><h2 id="AdaGrad-算法（Adaptive-Gradient-Algorithm）"><a href="#AdaGrad-算法（Adaptive-Gradient-Algorithm）" class="headerlink" title="AdaGrad 算法（Adaptive Gradient Algorithm）"></a>AdaGrad 算法（Adaptive Gradient Algorithm）</h2><p>沿着“陡峭”方向的进展受到抑制，而沿着“平坦”方向的进展被加速。</p><p><img src="/2025/03/29/deeplearning-note-part1/AdaGradpro.png" alt="AdaGrad 算法"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span>  <span class="comment"># 初始化梯度累积项</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">    dw = compute_gradient(w)  <span class="comment"># 计算梯度</span></span><br><span class="line">    grad_squared += dw * dw  <span class="comment"># 累积梯度平方</span></span><br><span class="line">    w -= learning_rate * dw / (grad_squared.sqrt() + <span class="number">1e-7</span>)  <span class="comment"># 进行参数更新，避免除零</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>问题：grad_squared 可能会在到达损失函数最低点过大，而使得参数停止更新。<br>解决方法：<strong>RMSProp</strong>。</p><h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p>与 AdaGrad 算法相比增加了一个“摩擦”项。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span>  <span class="comment"># 初始化累积梯度平方项</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">    dw = compute_gradient(w)  <span class="comment"># 计算梯度</span></span><br><span class="line">    grad_squared = decay_rate * grad_squared + (<span class="number">1</span> - decay_rate) * dw * dw  <span class="comment"># 计算加权移动平均</span></span><br><span class="line">    w -= learning_rate * dw / (grad_squared.sqrt() + <span class="number">1e-7</span>)  <span class="comment"># 更新参数</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Adam-算法"><a href="#Adam-算法" class="headerlink" title="Adam 算法"></a>Adam 算法</h2><p>结合两个好的 idea: SGD with Momentum + RMSProp = Adam<br>但是刚开始的时候梯度可能过大。<br>优化：<strong>偏差修正</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">moment1 = <span class="number">0</span>  <span class="comment"># 一阶矩估计（动量项）</span></span><br><span class="line">moment2 = <span class="number">0</span>  <span class="comment"># 二阶矩估计（梯度平方的指数加权平均）</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">    dw = compute_gradient(w)  <span class="comment"># 计算梯度</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算一阶矩估计（动量）</span></span><br><span class="line">    moment1 = beta1 * moment1 + (<span class="number">1</span> - beta1) * dw</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算二阶矩估计（梯度平方的移动平均）</span></span><br><span class="line">    moment2 = beta2 * moment2 + (<span class="number">1</span> - beta2) * dw * dw</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算一阶和二阶矩的偏差修正</span></span><br><span class="line">    moment1_unbias = moment1 / (<span class="number">1</span> - beta1 ** t)</span><br><span class="line">    moment2_unbias = moment2 / (<span class="number">1</span> - beta2 ** t)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    w -= learning_rate * moment1_unbias / (moment2_unbias.sqrt() + <span class="number">1e-7</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>Adam with beta1 = 0.9<br>beta2 = 0.999, and learning rate = 1e-3, 5e-4, 1e-4 is a great starting point for many models!</p></blockquote><h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><p>深度神经网络的层数通常是指网络所含权重矩阵的个数。<br>宽度是隐藏表示的纬度。隐藏表示的纬度通常是一样的。<br>激活函数可以看成两个权重矩阵之间的“三明治”，给予网络额外的表现能力。</p><p>激活函数有多种。<br><img src="/2025/03/29/deeplearning-note-part1/Activation%20Functions.png" alt="激活函数的种类"></p><p>通常应该使用某种可调正则化参数的神经网络模型，而不是直接依赖网络本身的大小作为正则化因子。<br>网络大小不一定是最优正则化方式：虽然较大的网络可能更容易过拟合，但仅仅减少参数数量并不总是最佳策略。</p><h2 id="特征变换"><a href="#特征变换" class="headerlink" title="特征变换"></a>特征变换</h2><p>通过对原始的输入特征进行变换，可能就能够处理原来的模型所不能处理的问题。对于图像分类而言，常用的特征变换包括：</p><ul><li>Color Histogram 色彩直方图</li><li>Histogram of Oriented Gradients (HoG) 方向梯度直方图 (HoG)</li><li>Bag of Words 词袋模型 <strong><em>数据驱动的</em></strong></li></ul><p>不同的特征变换可以组合在一起使用。</p><p><img src="/2025/03/29/deeplearning-note-part1/image%20Features%20vs%20Neural%20Networks.png" alt="image Features vs Neural Networks"></p><h2 id="端到端的学习"><a href="#端到端的学习" class="headerlink" title="端到端的学习"></a>端到端的学习</h2><p>先进行特征提取再利用模型在提取出的特征上对图像进行分类时候，只会调整模型的参数，而特征提取的部分可能不会提高图片的分类效果。<br>因此，更好的办法是端到端的学习，输入原始数据，输出想要的结果。中间整体地对特征提取和特征处理部分进行训练来提高图像分类效果。</p><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p><img src="/2025/03/29/deeplearning-note-part1/Backpropagation.png" alt="反向传播"><br>pytorch 中模型的计算步骤存储在<strong>计算图</strong>中，每个节点代表一次运算。<br>反向传播中，对于计算图中每个节点来说 downstream gradient = upstream gradient * local gradient<br>在代码中：正向传播与反向传播的代码通常一一对应，但是顺序相反。</p><h1 id="卷积网络"><a href="#卷积网络" class="headerlink" title="卷积网络"></a>卷积网络</h1><p>卷积网络中的权重矩阵一般称为 <strong>卷积核</strong> 或者 <strong>filter</strong>，它的深度一般和输入张量的深度一致，比如说 3。<br>输入张量和卷积核卷积后的结果被称为 <strong>activation map</strong>。<br>一层卷积层可以有多个卷积核，这是一个 可以设置的超参数。卷积核的个数等于<strong>输出中的通道数</strong>。<br><img src="/2025/03/29/deeplearning-note-part1/Convolutional%20filter.png" alt="卷积操作"></p><p>有两种方式看待卷积后的结果：</p><ol><li>一系列的 feature map 的集合。</li><li>特征向量组成的网格。</li></ol><p>通常对一批图像进行处理。<br><img src="/2025/03/29/deeplearning-note-part1/generlized%20Convolution%20computation.png" alt="一般的卷积操作"></p><blockquote><p>一般可以对第一层的参数可视化进行解释。</p></blockquote><h2 id="步幅和填充"><a href="#步幅和填充" class="headerlink" title="步幅和填充"></a>步幅和填充</h2><p>padding: 在图片周围填充来防止图片尺寸缩小<br>特例：<strong>same padding</strong>后图像的大小不会改变<br><img src="/2025/03/29/deeplearning-note-part1/same%20padding.png" alt="same padding"></p><p>stride:下采样，防止网络需要很多层卷积才能获取到输入图片的全局信息。<br>除了 conv 中的 stride 可以下采样，池化层也可以下采样。</p><p>卷积通常的参数设置：<br><img src="/2025/03/29/deeplearning-note-part1/convolution%20common%20setting.png" alt="卷积通常的参数设置"></p><h2 id="全连接层和-1x1-卷积的区别"><a href="#全连接层和-1x1-卷积的区别" class="headerlink" title="全连接层和 1x1 卷积的区别"></a>全连接层和 1x1 卷积的区别</h2><p>全连接层可以用来破坏空间结构，比如网络最后一层生成分数。<br>1x1 卷积用来调节通道深度。</p><h2 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h2><p>问题：网络很难训练。解决方法：归一化<br>通常使用 <strong>批量归一化(BatchNorm)</strong> ，使得每一层的输出符合均值为 0，方差为 1 的分布。<br>批量归一化最初是为了解决神经网络训练中的“内部协变量偏移（Internal Covariate Shift）”问题,不是专门为了防止过拟合而设计的，但它确实能在一定程度上减少过拟合，这是一种“副作用”。<br>批量归一化训练和推理时行为不一致。<br>训练时：<br><img src="/2025/03/29/deeplearning-note-part1/batch%20norm%20in%20train.png" alt="批量归一化"><br>推理时：<br><img src="/2025/03/29/deeplearning-note-part1/batch%20norm%20in%20test.png" alt="批量归一化"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于卷积层，批归一化在每个通道上单独进行</span></span><br><span class="line"><span class="comment"># 输入形状：(N, C, H, W)</span></span><br><span class="line"><span class="comment"># N：批次大小</span></span><br><span class="line"><span class="comment"># C：通道数</span></span><br><span class="line"><span class="comment"># H, W：特征图高度和宽度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对每个通道c：</span></span><br><span class="line">μc = mean(x[:, c, :, :])  <span class="comment"># 计算通道均值</span></span><br><span class="line">σ²c = var(x[:, c, :, :])  <span class="comment"># 计算通道方差</span></span><br><span class="line">x̂c = (x[:, c, :, :] - μc) / √(σ²c + ε)  <span class="comment"># ε是一个很小的数，防止除零</span></span><br><span class="line">yc = γc * x̂c + βc</span><br></pre></td></tr></table></figure><p><strong>批量归一化</strong> 中一个批次的样本之间相互影响，<strong>层归一化</strong>可以避免这一问题</p><h2 id="VGG-Net"><a href="#VGG-Net" class="headerlink" title="VGG Net"></a>VGG Net</h2><p>两个 3x3 的卷积比单个 5x5 的卷积在参数、浮点计算更低的情况下效果可能会更好。<br>用卷积 stage 替换卷积层。每个 stage 里有多个卷积。<br>通过减半空间大小和把通道数翻倍，保持每个卷积 stage 中浮点计算次数差不多。</p><blockquote><p>下采样：任何能够减少输入的空间尺寸的操作</p></blockquote><p>Tips: 在实际应用中，不应该自己设计新的网络架构，而是应该在现有好的网络基础上修改。<br><img src="/2025/03/29/deeplearning-note-part1/architecture%20choice.png" alt="网络选择"></p><h2 id="残差网络（ResNet）"><a href="#残差网络（ResNet）" class="headerlink" title="残差网络（ResNet）"></a>残差网络（ResNet）</h2><p>有了批量归一化以后可以训练很深的网络，但是模型由于<strong>欠拟合</strong>表现变差。<br><img src="/2025/03/29/deeplearning-note-part1/plainnetproblem.png" alt="普通网络问题"></p><p>ResNet 通过在卷积层之间加入一个 shortcut，缓解了梯度消失/爆炸的问题，提高了模型的稳定性和可优化性。<br>普通残差块由两个 3*3 的卷积层和捷径构成。<br><img src="/2025/03/29/deeplearning-note-part1/normresblock.png" alt="普通残差块"></p><p>还有另一种形式的残差块，称为瓶颈块(Bottleneck block)，由 1*1, 3*3, 1*1 的三个卷积层构成, 并在开头和结尾改变通道数。<br><img src="/2025/03/29/deeplearning-note-part1/Bottleneckblock.png" alt="瓶颈块"></p><p>解决问题时 resnet 效果通常不错，可以首先尝试一下。</p><h1 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h1><h2 id="静态计算图与动态计算图"><a href="#静态计算图与动态计算图" class="headerlink" title="静态计算图与动态计算图"></a>静态计算图与动态计算图</h2><p>静态计算图构建好后不会改变，动态计算图在每次前向传播中会构建新的计算图。</p><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><p>优化区别<br><img src="/2025/03/29/deeplearning-note-part1/staticvsdynamicop.png" alt="静态动态优化区别"><br>序列化区别<br><img src="/2025/03/29/deeplearning-note-part1/staticvsdynamicse.png" alt="静态动态序列化区别"><br>调试区别<br><img src="/2025/03/29/deeplearning-note-part1/staticvsdynamicde.png" alt="静态动态调试区别"></p><h2 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h2><p>Pytoch 有三个抽象层次：</p><ul><li>张量</li><li>自动微分</li><li>模块</li></ul><p><img src="/2025/03/29/deeplearning-note-part1/pytorchabstractionlevel.png" alt="pytorch抽象层次"></p><p>代码实例：<br><img src="/2025/03/29/deeplearning-note-part1/pytorchcodeexample.png" alt="pytorch代码实例"></p><p><code>with torch.no_grad():</code>告诉 pytoch 不要为上下文管理器中的操作构建计算图，通常<strong>梯度更新</strong>和<strong>置零</strong>不需要反向传播来计算梯度。</p><p>通过继承 nn.module 可以很方便地自定义网络。<br><img src="/2025/03/29/deeplearning-note-part1/customizemodule.png" alt="自定义网络"></p><p>pytorch 可以很方便地下载并使用预训练好的模型。<br><img src="/2025/03/29/deeplearning-note-part1/pretrainedmodels.png" alt="预训练好的模型"></p><p>pytorch 默认使用动态计算图。动态计算图使你可以在前向传播中使用控制语句，比如根据 loss 的不同选择为一个线性层选择不同的权重矩阵。<br><img src="/2025/03/29/deeplearning-note-part1/dynamicgraphpro.png" alt="动态图优点"></p><p>pytorch 可以使用静态图（也可以使用装饰器装饰 model 函数）。<br><img src="/2025/03/29/deeplearning-note-part1/staticgraphinpytorch.png" alt="pytorch静态图"></p><p>Pytoch 的张量操作中有任何一个输入张量的<code>require_grads</code>属性为<code>True</code>，pytorch 会为这个操作构建一部分计算图，并且操作的输出张量中<code>require_grads</code>属性也被 pytorch 设置为<code>True</code>。</p><h2 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h2><p><code>TensorFlow1.0</code>主要用静态计算图，<code>TensorFlow2.0</code>主要用动态计算图。</p><p>TensorFlow 中的<code>keras</code>类似于 pytoch 中的 nn 模块，提供模块级别的抽象。</p><p>TensorFlow 中的<code>tensorboard</code>很好用，是一个用来追踪网络统计信息的<code>web server</code>，pytorch 在<code>torch.utils.tensorboard</code>也提供了对 tensorboard 的支持。</p><h1 id="网络的训练"><a href="#网络的训练" class="headerlink" title="网络的训练"></a>网络的训练</h1><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>图像中的像素值(0-255)一般都是正数，梯度就都是正的或是负的，不利于梯度更新。</p><p>对于 <strong>图像</strong> ，可以把数据集移到中心，调整方差。<br><img src="/2025/03/29/deeplearning-note-part1/imagepreprocess.png" alt="图像的数据预处理"></p><p>对于 <strong>非图像</strong> ，可以旋转数据集，使得特征之间不相互关联。<br><img src="/2025/03/29/deeplearning-note-part1/nonimagepreprocess.png" alt="非图像的数据预处理"></p><p>所有必须保持一致的转换（如标准化、编码）在训练和测试都做，但用<strong>训练集</strong>的统计量。因为在真实的世界中，没有训练集给你，没办法计算统计量。</p><p>不同色彩空间的图片之间的转换关系简单，网络可以很容易学习到，通常用 RGB 空间即可。</p><p>常用于图像的数据预处理:<br><img src="/2025/03/29/deeplearning-note-part1/datapreprocessingimg.png" alt="常用于图像的数据预处理"></p><h2 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h2><blockquote><p>在神经网络中，<strong>激活值</strong> 就是神经元的输出(经过激活函数处理后)，反映的是每个神经元“有没有响应”、“响应强不强”。</p></blockquote><p>对于 ReLU 函数来说，如果激活值是 0，那么梯度就是 0。但是对于 tanh 和 sigmoid 来说不一定。</p><p>初始化的目的是 <strong>为了让梯度良好便于优化</strong> 。</p><p>用常数去初始化参数会使得梯度非常差，而用高斯分布去初始化网络仅适用于浅层网络，不适用于深层网络。可以采用 Xavier 和 MSRA 初始化。</p><h3 id="Xavier-初始化"><a href="#Xavier-初始化" class="headerlink" title="Xavier 初始化"></a>Xavier 初始化</h3><p>对于 tanh 激活函数可以用 Xavier 初始化。原理是保持前后层方差一致。</p><p>对于全连接层。高斯分布的标准差 std = 1/sqrt(Din)。对于卷积层来说 Din = 卷积核大小的平方*输入通道数</p><h3 id="MSRA-Kaiming-初始化"><a href="#MSRA-Kaiming-初始化" class="headerlink" title="MSRA(Kaiming) 初始化"></a>MSRA(Kaiming) 初始化</h3><blockquote><p>何恺明当时在 Microsoft Research Asia（微软亚洲研究院，简称 MSRA） 工作。</p></blockquote><p>对于 Relu 激活函数要进行修正，乘以 2。<br>高斯分布的标准差 std = 2 /sqrt(Din)</p><p>对于残差网络的参数来说:<br><img src="/2025/03/29/deeplearning-note-part1/residualinit.png" alt="残差网络参数初始化"></p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数定量描述模型的好坏，它代表了我们对于模型中参数的偏好。可能有两组参数的损失值相同，仅由数据计算而来的损失函数会认为两者相同。可以在损失函数中加入正则项来体现出人类的先验的对参数的偏好。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p><img src="/2025/03/29/deeplearning-note-part1/Activation%20Functions.png" alt="激活函数的种类"></p><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p>问题:</p><ol><li>饱和的神经元杀死梯度。当输入的 x 值过大和过小时候，Sigmoid 函数的 local gradient 非常小。</li><li>函数输出不是以零为中心。优化时候会走弯路，采用 minibatch 可以缓解这一点。</li><li>指数运算成本高。</li></ol><h3 id="Dead-ReLU"><a href="#Dead-ReLU" class="headerlink" title="Dead ReLU"></a>Dead ReLU</h3><p>当某个神经元（或者说某一层的某个输出通道）在训练过程中，<strong><em>无论</em></strong> 输入什么数据，它的输出永远是 0，那这个神经元就叫“死亡”了。</p><p>原因是：这个神经元的输入总是小于等于 0。</p><p>在 ReLU 的负区间，梯度是 0，反向传播时无法更新权重，那么训练过程中，这个神经元“永远不会再激活”。</p><p>采用 Leaky ReLU 可以避免这一问题，但是 Leaky ReLU 中有超参数，可以使网络自动学习这个参数。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="/2025/03/29/deeplearning-note-part1/activationfuncsummary.png" alt="激活函数总结"><br><strong>不要</strong>使用 Sigmoid 或者 tanh,现代的激活函数效果都差不多。</p><h2 id="超参数"><a href="#超参数" class="headerlink" title="超参数"></a>超参数</h2><h3 id="Learning-rate-schedule"><a href="#Learning-rate-schedule" class="headerlink" title="Learning rate schedule"></a>Learning rate schedule</h3><p>大多数优化算法中都有<strong>学习率</strong>的超参数，可以依据训练轮次设置不同的学习率。但常数学习率通常也是不错的选择。</p><p>一开始用较大学习率，可以让损失值快速降低。后期用较小学习率，可以使模型更稳定地收敛到最优解。</p><p>有一种余弦方式的学习率衰减。公式中需要设置迭代次数和初始学习率，实际上只需要给初始学习率即可。</p><p><img src="/2025/03/29/deeplearning-note-part1/lrcosine.png" alt="余弦学习率衰减"></p><p>Adam 算法中使用常数学习率效果就不错。</p><h3 id="搜索超参数"><a href="#搜索超参数" class="headerlink" title="搜索超参数"></a>搜索超参数</h3><p>选择超参数时候，网格搜索(grid search)和随机搜索(random search)经常使用。网格搜索指定具体值，随机搜索指定范围。通常都是对数线性区间内。<br>一般<strong>随机选择</strong>更好，因为不同超参数对模型表现影响程度可能不一样。</p><blockquote><p>如果一个数列在对数坐标系下等间距，那它就是“对数线性”的。如 [0.01, 0.1, 1, 10, 100]，每个值的“对数”相差相同。</p></blockquote><p>GPU 多如何选择：遍历。<br>GPU 少按以下方法选择：</p><ol><li>检查初始损失<br>关闭权重衰减，并在初始化时检查损失是否正常</li><li>在小样本上过拟合<br>在一小部分训练数据（大约 5~10 个 minibatch）上尝试将模型训练到 100% 准确率。调节网络结构、学习率、权重初始化。关闭所有正则化。<ul><li>如果损失没有下降：可能是学习率太低或初始化不好。</li><li>如果损失爆炸到 Inf 或 NaN：可能是学习率太高或初始化不好。</li></ul></li><li>找到能让损失下降的学习率<br>使用上一步调好的结构，加载全部训练数据，开启小的权重衰减，寻找一个能让损失在约 100 次迭代内明显下降的学习率<br>推荐尝试的学习率：1e-1, 1e-2, 1e-3, 1e-4。</li><li>粗略网格搜索，训练约 1~5 个 epoch<br>在第 3 步中表现较好的学习率和权重衰减值附近，选取几组组合进行训练（大约 1~5 个 epoch）<br>推荐尝试的权重衰减值：1e-4, 1e-5, 0</li><li>细化网格，进行更长时间训练<br>从第 4 步中挑选表现最好的模型，进行更长时间的训练（大约 10~20 个 epoch），此阶段不使用学习率衰减</li><li>观察学习曲线。</li></ol><p>6 中的学习曲线包括<strong>损失函数</strong>(散点图和移动平均)和<strong>准确率</strong>（训练集和验证集上）曲线，非常有用。<br><img src="/2025/03/29/deeplearning-note-part1/learningcurve.png" alt="学习曲线"></p><p>可能会遇到的学习曲线情况：<br><img src="/2025/03/29/deeplearning-note-part1/possiblelearningcurve.png" alt="可能会遇到的学习曲线"></p><p>有一些经验方法来查看是否出错, 例如参数的更新值/参数值应该在 1e-3 左右。</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>正则化的作用是在训练模型时防止过拟合，提高模型在新数据上的泛化能力。<br>训练模型时，有时候模型在训练集上表现很好（损失很小、准确率很高），但在测试集上表现很差。这就是过拟合（overfitting）。</p><p>正则化通常在训练时加入某种随机性，在测试时候平均掉这种随机性，比如 batch normalization。<br><img src="/2025/03/29/deeplearning-note-part1/regularizationcommonpatern.png" alt="正则化共性"></p><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>通常对于线性层会使用 Dropout 进行正则化。</p><p>Dropout 会以概率 p（通常是 0.5），在每次前向传播中，随机将某一层的每个神经元的激活值置为 0，即“丢弃”该神经元，使其在当前这一轮不参与计算。这避免了特征之间的过度依赖。另一种观点是 Dropout 是训练了一大堆权重相同的小模型，每个小模型是完整模型的一部分。</p><p>在测试时候，必须缩放激活值使得：测试时候的输出等于训练时候输出的均值。有一种逆 Dropout 是在训练时候缩放来减少测试时候对设备的算力要求。</p><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>对于图片来讲，通过随机剪切、翻转等操作可以引入随机性，但通常不被认为是一种正则化。数据增强种类非常多，可以根据问题引入合适的操作。数据增强可以扩大数据集。</p><h3 id="早停法（Early-Stopping）"><a href="#早停法（Early-Stopping）" class="headerlink" title="早停法（Early Stopping）"></a>早停法（Early Stopping）</h3><p><img src="/2025/03/29/deeplearning-note-part1/EarlyStopping.png" alt="早停法"></p><h3 id="批量归一化-1"><a href="#批量归一化-1" class="headerlink" title="批量归一化"></a>批量归一化</h3><p>见<a href="#批量归一化">卷积网络</a>章节中的介绍</p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>Resnet 以后，现在通常只用 l2 正则、batch normalization 和数据增强。</p><div class="table-container"><table><thead><tr><th style="text-align:center">网络层类型</th><th style="text-align:center">常用正则化方法</th></tr></thead><tbody><tr><td style="text-align:center">线性层</td><td style="text-align:center">Dropout</td></tr><tr><td style="text-align:center">卷积层</td><td style="text-align:center">批量归一化</td></tr><tr><td style="text-align:center">Transformer 块</td><td style="text-align:center">层归一化</td></tr></tbody></table></div><p><img src="/2025/03/29/deeplearning-note-part1/regularizationsummary.png" alt="正则化总结"></p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="网络中的张量"><a href="#网络中的张量" class="headerlink" title="网络中的张量"></a>网络中的张量</h3><h4 id="logits"><a href="#logits" class="headerlink" title="logits"></a>logits</h4><p>logits 是神经网络最后一层输出、在激活函数之前的值。</p><ul><li>多分类任务：用 softmax 把 logits 转换成概率分布。</li><li>二分类任务：用 sigmoid 作用在单个 logit 上，输出一个 0~1 之间的概率</li></ul><p>PyTorch 中的很多损失函数（如 BCEWithLogitsLoss，CrossEntropyLoss）都内置了数值稳定的版本，它们要求输入 logits 而不是概率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">loss_fn = nn.BCEWithLogitsLoss()  <span class="comment"># expects logits</span></span><br><span class="line">logits = model(x)  <span class="comment"># model 不要加 sigmoid</span></span><br><span class="line">loss = loss_fn(logits, targets)   <span class="comment"># loss 函数内部自动处理数值稳定的 sigmoid 和 log 运算</span></span><br></pre></td></tr></table></figure><h4 id="张量形状"><a href="#张量形状" class="headerlink" title="张量形状"></a>张量形状</h4><p>参数的梯度表示的是损失函数对该参数的偏导数，参数的梯度的 shape 与参数本身的 shape 是一般是<strong>一样</strong>的，这样在更新参数时才可以逐元素地进行更新。</p><ol><li><p>全连接层（Linear / Dense）</p><ul><li>输入：一个向量，比如大小为 (batch_size, input_dim)</li><li>层定义：nn.Linear(input_dim, output_dim)</li><li><p>输出：一个向量，形状为 (batch_size, output_dim)</p><blockquote><p><code>nn.Linear(input_dim, output_dim)</code>中神经元数量就等于<code>output_dim</code>。每个神经元参数数为<code>input_dim + 1</code>（权重 + 偏置）。每个神经元都接收所有 <code>input_dim</code>个输入特征。</p></blockquote></li></ul></li><li><p>卷积层（Conv2d）</p><ul><li>输入：图像/特征图，形状为 (batch_size, in_channels, H, W)</li><li>层定义：nn.Conv2d(in_channels, out_channels, kernel_size)</li><li>输出：形状为 (batch_size, out_channels, H_out, W_out)</li></ul></li><li><p>激活层（ReLU、Sigmoid、Tanh 等）</p><ul><li>不会改变维度，只是逐元素地变换张量的值。</li></ul></li><li><p>Flatten 层</p><ul><li>把 tensor 变成一个向量，用于送进全连接层。</li></ul></li></ol><h3 id="迭代（iteration）与轮次（epoch）"><a href="#迭代（iteration）与轮次（epoch）" class="headerlink" title="迭代（iteration）与轮次（epoch）"></a>迭代（iteration）与轮次（epoch）</h3><ul><li><strong>Epoch</strong>:数据集被完整地送入模型中训练一次（全体样本都训练过一次）</li><li><strong>Iteration</strong>:一次参数更新，也就是一个 batch 被送入模型训练一次</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):        <span class="comment"># 外层是 epoch</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> dataloader:          <span class="comment"># 内层是 iteration</span></span><br><span class="line">        ...</span><br><span class="line">        optimizer.step()              <span class="comment"># 每个 batch 更新一次参数 = 一个 iteration</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h3><p>集成多个模型和集成训练中<strong>模型的多个历史快照</strong>通常可以稍微提高模型表现。</p><h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h3><ol><li>可以把 CNN 作为特征提取器，去除最后一层，用 CNN 提取出的特征训练简单的线性分类器可以实现不错效果。</li><li>也可以微调模型，在新的数据集上继续训练 CNN 模型。</li></ol><p>通常在 Imagenet 上预训练模型，在运用到下游任务上可以取得不错的效果。</p><p><img src="/2025/03/29/deeplearning-note-part1/transferlearning.png" alt="迁移学习"></p><h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p>消融实验的基本思路是：“逐个移除或修改模型中的某个组件、模块、特征或机制，观察其对模型性能的影响。”</p><p>消融实验的结果通常会以表格形式展示：<br>模型版本 | 准确率 | F1 分数<br>| —- | :—————-:| :—-: |<br>完整模型 | 92.5% | 0.91<br>去掉注意力模块 | 89.3% | 0.87<br>去掉多尺度融合 | 90.1% | 0.88</p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://example.com">touchsky</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://example.com/2025/03/29/deeplearning-note-part1/">http://example.com/2025/03/29/deeplearning-note-part1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/avatar.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/04/10/deeplearning-note-part2/" title="深度学习笔记(第二部分)"><div class="cover" style="background:var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">深度学习笔记(第二部分)</div></div><div class="info-2"><div class="info-item-1">循环神经网络 (Recurrent Networks)应用场景之前 CNN 和 MLP 的输入和输出都只有一个,但实际问题中的输入和输出可能有多个。比如给图像加描述(one to many), 视频分类(many to one), 机器翻译(many to many),这些应用中可以使用循环神经网络。 当你处理的问题的输入或输出涉及到 序列 时候，可以使用循环神经网络。 循环神经网络可以对非序列型的数据比如图片进行序列化的处理。 基础概念我们可以通过在每一个时间步应用一个递推公式来处理一系列向量$x$。其中使用相同的权重矩阵$W$和状态更新函数$f_W$就可以处理任意长的序列。 在序列被处理过程中，RNN 有一个一直更新的内部状态。 h_{t}=f_{W} ( h_{t-1}, x_{t} )初始状态$h_0$被设置为全 0 或者通过学习得到。 RNN 实现Vanilla RNN一种 RNN 是”Vanilla RNN”。 h_{t}=t a n h ( W_{h h} h_{t-1}+W_{x h} x_{t}+B_{h}...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.webp" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">touchsky</div><div class="author-info-description">学习笔记、技术问题记录</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/touchsky-real" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8%EF%BC%88Linear-Classifiers%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">线性分类器（Linear Classifiers）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%EF%BC%88Optimization%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">优化（Optimization）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#SGD"><span class="toc-number">2.1.</span> <span class="toc-text">SGD</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%98"><span class="toc-number">2.1.1.</span> <span class="toc-text">问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SGD-with-Momentum"><span class="toc-number">2.2.</span> <span class="toc-text">SGD with Momentum</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Nesterov-Momentum"><span class="toc-number">2.3.</span> <span class="toc-text">Nesterov Momentum</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AdaGrad-%E7%AE%97%E6%B3%95%EF%BC%88Adaptive-Gradient-Algorithm%EF%BC%89"><span class="toc-number">2.4.</span> <span class="toc-text">AdaGrad 算法（Adaptive Gradient Algorithm）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RMSProp"><span class="toc-number">2.5.</span> <span class="toc-text">RMSProp</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adam-%E7%AE%97%E6%B3%95"><span class="toc-number">2.6.</span> <span class="toc-text">Adam 算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">3.</span> <span class="toc-text">神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="toc-number">3.1.</span> <span class="toc-text">基础概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E5%8F%98%E6%8D%A2"><span class="toc-number">3.2.</span> <span class="toc-text">特征变换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.3.</span> <span class="toc-text">端到端的学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">3.4.</span> <span class="toc-text">反向传播</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C"><span class="toc-number">4.</span> <span class="toc-text">卷积网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A5%E5%B9%85%E5%92%8C%E5%A1%AB%E5%85%85"><span class="toc-number">4.1.</span> <span class="toc-text">步幅和填充</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E5%92%8C-1x1-%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">4.2.</span> <span class="toc-text">全连接层和 1x1 卷积的区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">4.3.</span> <span class="toc-text">批量归一化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VGG-Net"><span class="toc-number">4.4.</span> <span class="toc-text">VGG Net</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%EF%BC%88ResNet%EF%BC%89"><span class="toc-number">4.5.</span> <span class="toc-text">残差网络（ResNet）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A1%86%E6%9E%B6"><span class="toc-number">5.</span> <span class="toc-text">框架</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%99%E6%80%81%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%8E%E5%8A%A8%E6%80%81%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">5.1.</span> <span class="toc-text">静态计算图与动态计算图</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8C%BA%E5%88%AB"><span class="toc-number">5.1.1.</span> <span class="toc-text">区别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pytorch"><span class="toc-number">5.2.</span> <span class="toc-text">Pytorch</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TensorFlow"><span class="toc-number">5.3.</span> <span class="toc-text">TensorFlow</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%AD%E7%BB%83"><span class="toc-number">6.</span> <span class="toc-text">网络的训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">6.1.</span> <span class="toc-text">数据预处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">6.2.</span> <span class="toc-text">参数初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Xavier-%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">6.2.1.</span> <span class="toc-text">Xavier 初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MSRA-Kaiming-%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">6.2.2.</span> <span class="toc-text">MSRA(Kaiming) 初始化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">6.3.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">6.4.</span> <span class="toc-text">激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sigmoid"><span class="toc-number">6.4.1.</span> <span class="toc-text">Sigmoid</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dead-ReLU"><span class="toc-number">6.4.2.</span> <span class="toc-text">Dead ReLU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">6.4.3.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-number">6.5.</span> <span class="toc-text">超参数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Learning-rate-schedule"><span class="toc-number">6.5.1.</span> <span class="toc-text">Learning rate schedule</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%90%9C%E7%B4%A2%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-number">6.5.2.</span> <span class="toc-text">搜索超参数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">6.6.</span> <span class="toc-text">正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout"><span class="toc-number">6.6.1.</span> <span class="toc-text">Dropout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">6.6.2.</span> <span class="toc-text">数据增强</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A9%E5%81%9C%E6%B3%95%EF%BC%88Early-Stopping%EF%BC%89"><span class="toc-number">6.6.3.</span> <span class="toc-text">早停法（Early Stopping）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96-1"><span class="toc-number">6.6.4.</span> <span class="toc-text">批量归一化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-number">6.6.5.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">6.7.</span> <span class="toc-text">其他</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%BC%A0%E9%87%8F"><span class="toc-number">6.7.1.</span> <span class="toc-text">网络中的张量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#logits"><span class="toc-number">6.7.1.1.</span> <span class="toc-text">logits</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%BD%A2%E7%8A%B6"><span class="toc-number">6.7.1.2.</span> <span class="toc-text">张量形状</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%AD%E4%BB%A3%EF%BC%88iteration%EF%BC%89%E4%B8%8E%E8%BD%AE%E6%AC%A1%EF%BC%88epoch%EF%BC%89"><span class="toc-number">6.7.2.</span> <span class="toc-text">迭代（iteration）与轮次（epoch）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%9B%86%E6%88%90"><span class="toc-number">6.7.3.</span> <span class="toc-text">模型集成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="toc-number">6.7.4.</span> <span class="toc-text">迁移学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-number">6.7.5.</span> <span class="toc-text">消融实验</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/10/deeplearning-note-part2/" title="深度学习笔记(第二部分)">深度学习笔记(第二部分)</a><time datetime="2025-04-10T12:40:16.000Z" title="Created 2025-04-10 20:40:16">2025-04-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/29/deeplearning-note-part1/" title="深度学习笔记(第一部分)">深度学习笔记(第一部分)</a><time datetime="2025-03-29T12:59:13.000Z" title="Created 2025-03-29 20:59:13">2025-03-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By touchsky</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(()=>{const t=()=>{if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typesetPromise();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"none"},chtml:{scale:1.1},options:{enableMenu:!0,renderActions:{findScript:[10,t=>{for(const e of document.querySelectorAll('script[type^="math/tex"]')){const n=!!e.type.match(/; *mode=display/),a=new t.options.MathItem(e.textContent,t.inputJax[0],n),d=document.createTextNode("");e.parentNode.replaceChild(d,e),a.start={node:d,delim:"",n:0},a.end={node:d,delim:"",n:0},t.math.push(a)}},""]}}};const t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js",t.id="MathJax-script",t.async=!0,document.head.appendChild(t)}};btf.addGlobalFn("encrypt",t,"mathjax"),window.pjax?t():window.addEventListener("load",t)})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>